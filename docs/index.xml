<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Half-bayesianism</title>
    <link>https://ewerlopes.github.io/</link>
    <description>Recent content on Half-bayesianism</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://ewerlopes.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The reasonable power of the KL divergence</title>
      <link>https://ewerlopes.github.io/posts/2023/2023-10-24-kulback-leibler/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ewerlopes.github.io/posts/2023/2023-10-24-kulback-leibler/</guid>
      <description>When doing Bayesian Inference, or any other type of inference, chances are you have heard about KL divergence &amp;ndash; short for Kullback-Leibler divergence. This quantity has been pervasive in machine learning and artificial intelligence and in this post I would like to explore with you, the reader, some reasons for why this is so, especially for probabilistic inference. Eventually, we&amp;rsquo;ll find ourselves dealing with Variational Inference and with a little more patience, with Expectation Propagation.</description>
    </item>
    
    <item>
      <title>My recommended research methodology</title>
      <link>https://ewerlopes.github.io/posts/2021/2021-11-01-research-methodology/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ewerlopes.github.io/posts/2021/2021-11-01-research-methodology/</guid>
      <description>Fact is: research is a complex process. When done right, it is just art; when done poorly, it may have far-reaching consequences across multiple dimensions; it can bring one to a hideous spiral of psychological issues.
Right! Let&amp;rsquo;s get down to business.
We all have heard people talk about the importance of time management. Rarely this subject, like proper civil education, is discussed with greater detail or practice during formal training.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://ewerlopes.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ewerlopes.github.io/about/</guid>
      <description>ðŸŽ¯ My Interests I have a deep interest in the probabilistic perspective to machine learning and artificial intelligence, especially in the identification of causal mechanisms via statistical quantities from data (aka Causal Inference). Other topics of interest include (but are not limited to):
 Learning Theory (bandits, game theory, statistical learning theory, etc.) Optimization (convex and non-convex optimization, matrix/tensor methods, sparsity, etc.) Probabilistic Inference (Bayesian methods, graphical models, Monte Carlo methods, etc.</description>
    </item>
    
  </channel>
</rss>
